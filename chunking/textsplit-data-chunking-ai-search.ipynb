{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure AI Search test splitter data chunking example\n",
    "\n",
    "This notebook uses the Text Split skill in Azure AI Search to chunk text. This approach takes a dependency on indexers and skillsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sithukaungset/Azure-AI-Search-prompthon/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load .env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "search_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "search_index = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "search_datasource = os.environ[\"AZURE_SEARCH_DATASOURCE\"]\n",
    "search_skillset = os.environ[\"AZURE_SEARCH_SKILLSET\"]\n",
    "search_indexer = os.environ[\"AZURE_SEARCH_INDEXER\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_embedding_deployment_id = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_ID\"]\n",
    "blob_container = os.environ[\"AZURE_BLOB_CONTAINER_NAME\"]\n",
    "blob_connection_string = os.environ[\"AZURE_BLOB_CONNECTION_STRING\"]\n",
    "blob_account_url = os.environ[\"AZURE_BLOB_ACCOUNT_URL\"]\n",
    "\n",
    "search_credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload sample PDF for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "def open_blob_client():\n",
    "    # Set max_block_size and max_single_put_size due to large PDF transfers\n",
    "    # See https://learn.microsoft.com/azure/storage/blobs/storage-blobs-tune-upload-download-python\n",
    "    if not blob_connection_string.startswith(\"ResourceId\"):\n",
    "        return BlobServiceClient.from_connection_string(\n",
    "            blob_connection_string,\n",
    "            max_block_size=1024*1024*8, # 8 MiB\n",
    "            max_single_put_size=1024*1024*8 # 8 MiB\n",
    "        )\n",
    "    return BlobServiceClient(\n",
    "        account_url=blob_account_url,\n",
    "        credential=DefaultAzureCredential(),\n",
    "        max_block_size=1024*1024*8, # 8 MiB\n",
    "        max_single_put_size=1024*1024*8 # 8 MiB\n",
    "    )\n",
    "\n",
    "blob_client = open_blob_client()\n",
    "container_client = blob_client.get_container_client(blob_container)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "file_path = os.path.join(\"..\",\"data\", \"role_library.pdf\")\n",
    "blob_name = os.path.basename(file_path)\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "if not blob_client.exists():\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        blob_client.upload_blob(data=f, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17997.57s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lib in /Users/sithukaungset/Azure-AI-Search-prompthon/venv/lib/python3.10/site-packages (4.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sithukaungset/Azure-AI-Search-prompthon/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup sample resources for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SearchIndexClient, SearchIndexerClient\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     create_search_index,\n\u001b[1;32m      4\u001b[0m     create_search_datasource,\n\u001b[1;32m      5\u001b[0m     create_search_skillset,\n\u001b[1;32m      6\u001b[0m     create_search_indexer\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m search_index_client \u001b[38;5;241m=\u001b[39m SearchIndexClient(endpoint\u001b[38;5;241m=\u001b[39msearch_endpoint, credential\u001b[38;5;241m=\u001b[39msearch_credential)\n\u001b[1;32m     10\u001b[0m index \u001b[38;5;241m=\u001b[39m create_search_index(\n\u001b[1;32m     11\u001b[0m     search_index,\n\u001b[1;32m     12\u001b[0m     azure_openai_endpoint,\n\u001b[1;32m     13\u001b[0m     azure_openai_embedding_deployment_id,\n\u001b[1;32m     14\u001b[0m     azure_openai_key\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Azure-AI-Search-prompthon/chunking/lib/common.py:29\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_generated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     SearchIndexerSkillset,\n\u001b[1;32m     20\u001b[0m     AzureOpenAIVectorizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     OutputFieldMappingEntry\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from lib.common import (\n",
    "    create_search_index,\n",
    "    create_search_datasource,\n",
    "    create_search_skillset,\n",
    "    create_search_indexer\n",
    ")\n",
    "\n",
    "search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "index = create_search_index(\n",
    "    search_index,\n",
    "    azure_openai_endpoint,\n",
    "    azure_openai_embedding_deployment_id,\n",
    "    azure_openai_key\n",
    ")\n",
    "search_index_client.create_or_update_index(index)\n",
    "\n",
    "search_indexer_client = SearchIndexerClient(endpoint=search_endpoint, credential=search_credential)\n",
    "\n",
    "data_source = create_search_datasource(\n",
    "    search_datasource,\n",
    "    blob_connection_string,\n",
    "    blob_container\n",
    ")\n",
    "search_indexer_client.create_or_update_data_source_connection(data_source)\n",
    "\n",
    "skillset = create_search_skillset(\n",
    "    search_skillset,\n",
    "    search_index,\n",
    "    azure_openai_endpoint,\n",
    "    azure_openai_embedding_deployment_id,\n",
    "    azure_openai_key,\n",
    "    text_split_mode='pages',\n",
    "    maximum_page_length=2000,\n",
    "    page_overlap_length=500\n",
    ")\n",
    "search_indexer_client.create_or_update_skillset(skillset)\n",
    "\n",
    "indexer = create_search_indexer(\n",
    "    indexer_name=search_indexer,\n",
    "    index_name=search_index,\n",
    "    datasource_name=search_datasource,\n",
    "    skillset_name=search_skillset\n",
    ")\n",
    "search_indexer_client.create_or_update_indexer(indexer)\n",
    "search_indexer_client.run_indexer(search_indexer)\n",
    "\n",
    "print(\"Running indexer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show chunk character length and token length histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lib.common import (\n",
    "    get_chunks,\n",
    "    get_token_length,\n",
    "    plot_chunk_histogram\n",
    ")\n",
    "\n",
    "search_client = search_index_client.get_search_client(search_index)\n",
    "chunks = get_chunks(search_client)\n",
    "\n",
    "plot_chunk_histogram(chunks, length_fn=len, title=\"Distribution of Chunk Character Length\", xlabel=\"Chunk Character Length\")\n",
    "plot_chunk_histogram(chunks, length_fn=get_token_length, title=\"Distribution of Chunk Token Length\", xlabel=\"Chunk Token Length\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
